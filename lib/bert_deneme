from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the fine-tuned model
model_path = "./fine_tuned_bert"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

def preprocess_text(text):
    return tokenizer(text, padding="max_length", truncation=True, max_length=128, return_tensors="pt")

import torch

def predict(text):
    inputs = preprocess_text(text)
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=-1)
    return predictions.item()

# Example usage
text = "This is a great product!"
prediction = predict(text)
print(f"Prediction: {prediction}")
